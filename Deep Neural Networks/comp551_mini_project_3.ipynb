{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of COMP-551-P3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVo9DCbSbLps"
      },
      "source": [
        "## **Import statements + set up device**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVupFgNcbNiw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91f4d0c3-514e-4faa-f132-989ef95ccb6c"
      },
      "source": [
        "# mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlEs_p-wbStY"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from random import randrange\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zufheW8bYB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946ee6fb-2ded-436a-fe99-57f01234366b"
      },
      "source": [
        "# use a gpu if it is available\n",
        "# Training on GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, the following should print a CUDA device\n",
        "print(f\"Currently runninng on: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently runninng on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMiknC3fbqWG"
      },
      "source": [
        "## **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR4VRP6obxse"
      },
      "source": [
        "# Hyper-parameters \n",
        "num_epochs = 50\n",
        "batch_size = 15\n",
        "learning_rate = 0.001\n",
        "validation_data_size = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr63_KZ-cDbV"
      },
      "source": [
        "## **Prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bb-n9vYcIKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ce39ec-f1d1-4331-d685-87bc963ab73e"
      },
      "source": [
        "# load training data\n",
        "training_images = \"/content/drive/My Drive/Colab Notebooks/images_l.pkl\"\n",
        "training_labels = \"/content/drive/My Drive/Colab Notebooks/labels_l.pkl\"\n",
        "\n",
        "with open(training_images, 'rb') as f: \n",
        "    X_train = pickle.load(f)\n",
        "\n",
        "with open(training_labels, 'rb') as f: \n",
        "    y_train = pickle.load(f)\n",
        "\n",
        "# Determine size of split for training and validation // Example = 24000 and 7000\n",
        "training_data_size = 1.0-validation_data_size\n",
        "training_data_size = (int) ((1.0-validation_data_size)*X_train.shape[0])\n",
        "validation_data_size = (int) (X_train.shape[0] - training_data_size)\n",
        "print(f'Size of training data    : {training_data_size}')\n",
        "print(f'Size of validation data  : {validation_data_size}')\n",
        "\n",
        "# need to ensure that 1 channel is specified - need this later when applying conv layer\n",
        "X_train = X_train.reshape(30000,1,56,56)\n",
        "# covert data to float tensors \n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "\n",
        "print(f\"Size of features :    {X_train.shape}\")\n",
        "print(f\"Size of labels   :    {y_train.shape}\")\n",
        "\n",
        "# create a dataset out of all the data\n",
        "all_data = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "\n",
        "# split randomly between training and validation data\n",
        "training_data , validation_data = torch.utils.data.random_split(all_data,[training_data_size,validation_data_size])\n",
        "\n",
        "# create train and validation loaders with specified batch size (this is a hyperparameter we can tweak)\n",
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True) \n",
        "val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of training data    : 24000\n",
            "Size of validation data  : 6000\n",
            "Size of features :    torch.Size([30000, 1, 56, 56])\n",
            "Size of labels   :    torch.Size([30000, 36])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oViCwbo8eE7Y"
      },
      "source": [
        "## **CNN Architecture and Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlrKwOPHeXTE"
      },
      "source": [
        "# method to train model using a dataloader\n",
        "def train_model(dataloader):\n",
        "  print(\"Training model:\")\n",
        "  train_losses = []\n",
        "  n_total_steps = len(dataloader)\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader):\n",
        "      \n",
        "      images,labels = data\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "                        \n",
        "      # Forward pass:\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "      train_losses.append(loss)\n",
        "\n",
        "      # Backward pass - compute gradients:\n",
        "      optimizer.zero_grad() # empty gradients\n",
        "      loss.backward() \n",
        "      optimizer.step()\n",
        "      # print usefull information while training:\n",
        "      if (i+1) % 500 == 0:\n",
        "        print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjwyZInyeJv7"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "    self.batchnorm16 = nn.BatchNorm2d(16)\n",
        "    self.batchnorm32 = nn.BatchNorm2d(32)\n",
        "    self.conv1 = nn.Conv2d(1, 16, 3)\n",
        "    self.conv2 = nn.Conv2d(16, 16, 3, padding='same')\n",
        "    self.conv3 = nn.Conv2d(16, 16, 3, padding='same')\n",
        "    self.conv4 = nn.Conv2d(16, 32, 3, padding='same')\n",
        "    self.conv5 = nn.Conv2d(32, 32, 3, padding='same')\n",
        "    self.conv6 = nn.Conv2d(32, 32, 3, padding='same')\n",
        "    self.conv7 = nn.Conv2d(32, 32, 3, padding='same')\n",
        "    self.conv8 = nn.Conv2d(32, 32, 3, padding='same')\n",
        "    self.fc1 = nn.Linear(32 * 3 * 3, 144)\n",
        "    self.fc2 = nn.Linear(144, 108)\n",
        "    self.fc3 = nn.Linear(108, 36)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.batchnorm16(x)\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.batchnorm16(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.batchnorm16(x)\n",
        "    x = self.pool(F.relu(self.conv4(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = self.batchnorm32(x)\n",
        "    x = F.relu(self.conv5(x))\n",
        "    x = self.batchnorm32(x)\n",
        "    x = self.pool(F.relu(self.conv6(x)))\n",
        "    x = self.batchnorm32(x)\n",
        "    x = F.relu(self.conv7(x))\n",
        "    x = self.pool(F.relu(self.conv8(x)))\n",
        "    x = self.batchnorm32(x)\n",
        "    x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# initialize our model\n",
        "model = ConvNet().to(device)\n",
        "# define the cross entropy loss function - note that this automatically peforms softmax\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HDwjtV3ep_J",
        "outputId": "a344bc48-e3b1-4036-cdff-18f4ef9374b1"
      },
      "source": [
        "# call model on training loader with labeled data:\n",
        "train_model(train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:\n",
            "Epoch [1/50], Step [500/1600], Loss: 6.3830\n",
            "Epoch [1/50], Step [1000/1600], Loss: 6.2443\n",
            "Epoch [1/50], Step [1500/1600], Loss: 5.6455\n",
            "Epoch [2/50], Step [500/1600], Loss: 5.0236\n",
            "Epoch [2/50], Step [1000/1600], Loss: 4.6667\n",
            "Epoch [2/50], Step [1500/1600], Loss: 4.2530\n",
            "Epoch [3/50], Step [500/1600], Loss: 4.9059\n",
            "Epoch [3/50], Step [1000/1600], Loss: 3.1690\n",
            "Epoch [3/50], Step [1500/1600], Loss: 3.9516\n",
            "Epoch [4/50], Step [500/1600], Loss: 3.0304\n",
            "Epoch [4/50], Step [1000/1600], Loss: 3.4377\n",
            "Epoch [4/50], Step [1500/1600], Loss: 2.7501\n",
            "Epoch [5/50], Step [500/1600], Loss: 2.5818\n",
            "Epoch [5/50], Step [1000/1600], Loss: 2.5652\n",
            "Epoch [5/50], Step [1500/1600], Loss: 3.0522\n",
            "Epoch [6/50], Step [500/1600], Loss: 2.4434\n",
            "Epoch [6/50], Step [1000/1600], Loss: 3.4105\n",
            "Epoch [6/50], Step [1500/1600], Loss: 2.0616\n",
            "Epoch [7/50], Step [500/1600], Loss: 2.2647\n",
            "Epoch [7/50], Step [1000/1600], Loss: 2.5204\n",
            "Epoch [7/50], Step [1500/1600], Loss: 3.5450\n",
            "Epoch [8/50], Step [500/1600], Loss: 2.4953\n",
            "Epoch [8/50], Step [1000/1600], Loss: 2.0808\n",
            "Epoch [8/50], Step [1500/1600], Loss: 2.1260\n",
            "Epoch [9/50], Step [500/1600], Loss: 2.5715\n",
            "Epoch [9/50], Step [1000/1600], Loss: 3.0649\n",
            "Epoch [9/50], Step [1500/1600], Loss: 1.9420\n",
            "Epoch [10/50], Step [500/1600], Loss: 2.5975\n",
            "Epoch [10/50], Step [1000/1600], Loss: 2.5527\n",
            "Epoch [10/50], Step [1500/1600], Loss: 2.0218\n",
            "Epoch [11/50], Step [500/1600], Loss: 1.8375\n",
            "Epoch [11/50], Step [1000/1600], Loss: 2.3886\n",
            "Epoch [11/50], Step [1500/1600], Loss: 2.1327\n",
            "Epoch [12/50], Step [500/1600], Loss: 2.1168\n",
            "Epoch [12/50], Step [1000/1600], Loss: 2.2007\n",
            "Epoch [12/50], Step [1500/1600], Loss: 1.6903\n",
            "Epoch [13/50], Step [500/1600], Loss: 2.0776\n",
            "Epoch [13/50], Step [1000/1600], Loss: 1.9228\n",
            "Epoch [13/50], Step [1500/1600], Loss: 2.0079\n",
            "Epoch [14/50], Step [500/1600], Loss: 1.5908\n",
            "Epoch [14/50], Step [1000/1600], Loss: 2.4055\n",
            "Epoch [14/50], Step [1500/1600], Loss: 1.8670\n",
            "Epoch [15/50], Step [500/1600], Loss: 2.1203\n",
            "Epoch [15/50], Step [1000/1600], Loss: 1.6733\n",
            "Epoch [15/50], Step [1500/1600], Loss: 1.8466\n",
            "Epoch [16/50], Step [500/1600], Loss: 1.9196\n",
            "Epoch [16/50], Step [1000/1600], Loss: 2.3493\n",
            "Epoch [16/50], Step [1500/1600], Loss: 1.6279\n",
            "Epoch [17/50], Step [500/1600], Loss: 1.9930\n",
            "Epoch [17/50], Step [1000/1600], Loss: 2.0851\n",
            "Epoch [17/50], Step [1500/1600], Loss: 2.2854\n",
            "Epoch [18/50], Step [500/1600], Loss: 1.7984\n",
            "Epoch [18/50], Step [1000/1600], Loss: 1.8724\n",
            "Epoch [18/50], Step [1500/1600], Loss: 2.3494\n",
            "Epoch [19/50], Step [500/1600], Loss: 2.2083\n",
            "Epoch [19/50], Step [1000/1600], Loss: 1.7971\n",
            "Epoch [19/50], Step [1500/1600], Loss: 1.6522\n",
            "Epoch [20/50], Step [500/1600], Loss: 1.6606\n",
            "Epoch [20/50], Step [1000/1600], Loss: 1.9251\n",
            "Epoch [20/50], Step [1500/1600], Loss: 1.6946\n",
            "Epoch [21/50], Step [500/1600], Loss: 2.0215\n",
            "Epoch [21/50], Step [1000/1600], Loss: 1.7066\n",
            "Epoch [21/50], Step [1500/1600], Loss: 2.0070\n",
            "Epoch [22/50], Step [500/1600], Loss: 1.7108\n",
            "Epoch [22/50], Step [1000/1600], Loss: 1.8573\n",
            "Epoch [22/50], Step [1500/1600], Loss: 1.8356\n",
            "Epoch [23/50], Step [500/1600], Loss: 2.1729\n",
            "Epoch [23/50], Step [1000/1600], Loss: 1.8700\n",
            "Epoch [23/50], Step [1500/1600], Loss: 1.7774\n",
            "Epoch [24/50], Step [500/1600], Loss: 1.7301\n",
            "Epoch [24/50], Step [1000/1600], Loss: 2.7780\n",
            "Epoch [24/50], Step [1500/1600], Loss: 1.7604\n",
            "Epoch [25/50], Step [500/1600], Loss: 1.9809\n",
            "Epoch [25/50], Step [1000/1600], Loss: 1.6809\n",
            "Epoch [25/50], Step [1500/1600], Loss: 1.5156\n",
            "Epoch [26/50], Step [500/1600], Loss: 2.0943\n",
            "Epoch [26/50], Step [1000/1600], Loss: 1.6562\n",
            "Epoch [26/50], Step [1500/1600], Loss: 1.6735\n",
            "Epoch [27/50], Step [500/1600], Loss: 2.1221\n",
            "Epoch [27/50], Step [1000/1600], Loss: 1.8544\n",
            "Epoch [27/50], Step [1500/1600], Loss: 1.7589\n",
            "Epoch [28/50], Step [500/1600], Loss: 2.1228\n",
            "Epoch [28/50], Step [1000/1600], Loss: 2.0920\n",
            "Epoch [28/50], Step [1500/1600], Loss: 1.7482\n",
            "Epoch [29/50], Step [500/1600], Loss: 1.7229\n",
            "Epoch [29/50], Step [1000/1600], Loss: 1.8437\n",
            "Epoch [29/50], Step [1500/1600], Loss: 1.6540\n",
            "Epoch [30/50], Step [500/1600], Loss: 1.4982\n",
            "Epoch [30/50], Step [1000/1600], Loss: 1.7895\n",
            "Epoch [30/50], Step [1500/1600], Loss: 1.6275\n",
            "Epoch [31/50], Step [500/1600], Loss: 1.6263\n",
            "Epoch [31/50], Step [1000/1600], Loss: 1.5990\n",
            "Epoch [31/50], Step [1500/1600], Loss: 2.3738\n",
            "Epoch [32/50], Step [500/1600], Loss: 1.5287\n",
            "Epoch [32/50], Step [1000/1600], Loss: 2.5874\n",
            "Epoch [32/50], Step [1500/1600], Loss: 1.6345\n",
            "Epoch [33/50], Step [500/1600], Loss: 2.3172\n",
            "Epoch [33/50], Step [1000/1600], Loss: 2.4664\n",
            "Epoch [33/50], Step [1500/1600], Loss: 1.7828\n",
            "Epoch [34/50], Step [500/1600], Loss: 1.9853\n",
            "Epoch [34/50], Step [1000/1600], Loss: 2.3682\n",
            "Epoch [34/50], Step [1500/1600], Loss: 1.6956\n",
            "Epoch [35/50], Step [500/1600], Loss: 1.6023\n",
            "Epoch [35/50], Step [1000/1600], Loss: 1.5240\n",
            "Epoch [35/50], Step [1500/1600], Loss: 1.4962\n",
            "Epoch [36/50], Step [500/1600], Loss: 1.8112\n",
            "Epoch [36/50], Step [1000/1600], Loss: 2.8528\n",
            "Epoch [36/50], Step [1500/1600], Loss: 1.8851\n",
            "Epoch [37/50], Step [500/1600], Loss: 1.7119\n",
            "Epoch [37/50], Step [1000/1600], Loss: 1.7476\n",
            "Epoch [37/50], Step [1500/1600], Loss: 1.6997\n",
            "Epoch [38/50], Step [500/1600], Loss: 1.6427\n",
            "Epoch [38/50], Step [1000/1600], Loss: 1.7670\n",
            "Epoch [38/50], Step [1500/1600], Loss: 1.4916\n",
            "Epoch [39/50], Step [500/1600], Loss: 2.2066\n",
            "Epoch [39/50], Step [1000/1600], Loss: 1.4668\n",
            "Epoch [39/50], Step [1500/1600], Loss: 2.0917\n",
            "Epoch [40/50], Step [500/1600], Loss: 1.8824\n",
            "Epoch [40/50], Step [1000/1600], Loss: 2.3504\n",
            "Epoch [40/50], Step [1500/1600], Loss: 1.5513\n",
            "Epoch [41/50], Step [500/1600], Loss: 2.0207\n",
            "Epoch [41/50], Step [1000/1600], Loss: 1.6711\n",
            "Epoch [41/50], Step [1500/1600], Loss: 1.8308\n",
            "Epoch [42/50], Step [500/1600], Loss: 1.9679\n",
            "Epoch [42/50], Step [1000/1600], Loss: 1.5105\n",
            "Epoch [42/50], Step [1500/1600], Loss: 1.6391\n",
            "Epoch [43/50], Step [500/1600], Loss: 1.7414\n",
            "Epoch [43/50], Step [1000/1600], Loss: 1.4802\n",
            "Epoch [43/50], Step [1500/1600], Loss: 1.9668\n",
            "Epoch [44/50], Step [500/1600], Loss: 1.7130\n",
            "Epoch [44/50], Step [1000/1600], Loss: 1.9453\n",
            "Epoch [44/50], Step [1500/1600], Loss: 1.6197\n",
            "Epoch [45/50], Step [500/1600], Loss: 1.8676\n",
            "Epoch [45/50], Step [1000/1600], Loss: 1.7001\n",
            "Epoch [45/50], Step [1500/1600], Loss: 1.6652\n",
            "Epoch [46/50], Step [500/1600], Loss: 2.2138\n",
            "Epoch [46/50], Step [1000/1600], Loss: 1.7830\n",
            "Epoch [46/50], Step [1500/1600], Loss: 1.6586\n",
            "Epoch [47/50], Step [500/1600], Loss: 2.1609\n",
            "Epoch [47/50], Step [1000/1600], Loss: 1.5137\n",
            "Epoch [47/50], Step [1500/1600], Loss: 1.7832\n",
            "Epoch [48/50], Step [500/1600], Loss: 1.6302\n",
            "Epoch [48/50], Step [1000/1600], Loss: 1.6271\n",
            "Epoch [48/50], Step [1500/1600], Loss: 1.5360\n",
            "Epoch [49/50], Step [500/1600], Loss: 1.6398\n",
            "Epoch [49/50], Step [1000/1600], Loss: 1.5824\n",
            "Epoch [49/50], Step [1500/1600], Loss: 1.6141\n",
            "Epoch [50/50], Step [500/1600], Loss: 1.5463\n",
            "Epoch [50/50], Step [1000/1600], Loss: 1.8007\n",
            "Epoch [50/50], Step [1500/1600], Loss: 2.0668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQefT-5yhepX"
      },
      "source": [
        "## **Data Augmentation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN62TqqthiTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5319ef15-188c-40e7-8fa6-337f74421305"
      },
      "source": [
        "# create a transform to randomly rotate data to +- 10\n",
        "rotate_transform = transforms.Compose([transforms.RandomRotation(degrees=10)])\n",
        "X_train_rotated = rotate_transform(X_train)\n",
        "# create new dataloader from rotated images and call model on rotated data to retrain\n",
        "rotated_data = torch.utils.data.TensorDataset(X_train_rotated,y_train)\n",
        "rotated_loader = torch.utils.data.DataLoader(rotated_data, batch_size=batch_size, shuffle=True)\n",
        "train_model(rotated_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model:\n",
            "Epoch [1/50], Step [500/2000], Loss: 1.7745\n",
            "Epoch [1/50], Step [1000/2000], Loss: 2.0897\n",
            "Epoch [1/50], Step [1500/2000], Loss: 2.3947\n",
            "Epoch [1/50], Step [2000/2000], Loss: 2.4575\n",
            "Epoch [2/50], Step [500/2000], Loss: 2.0953\n",
            "Epoch [2/50], Step [1000/2000], Loss: 1.5458\n",
            "Epoch [2/50], Step [1500/2000], Loss: 2.2615\n",
            "Epoch [2/50], Step [2000/2000], Loss: 2.8603\n",
            "Epoch [3/50], Step [500/2000], Loss: 2.3641\n",
            "Epoch [3/50], Step [1000/2000], Loss: 1.6477\n",
            "Epoch [3/50], Step [1500/2000], Loss: 1.5602\n",
            "Epoch [3/50], Step [2000/2000], Loss: 1.8039\n",
            "Epoch [4/50], Step [500/2000], Loss: 2.0462\n",
            "Epoch [4/50], Step [1000/2000], Loss: 1.6815\n",
            "Epoch [4/50], Step [1500/2000], Loss: 1.8796\n",
            "Epoch [4/50], Step [2000/2000], Loss: 1.7602\n",
            "Epoch [5/50], Step [500/2000], Loss: 1.7743\n",
            "Epoch [5/50], Step [1000/2000], Loss: 1.8263\n",
            "Epoch [5/50], Step [1500/2000], Loss: 1.8389\n",
            "Epoch [5/50], Step [2000/2000], Loss: 1.6509\n",
            "Epoch [6/50], Step [500/2000], Loss: 2.1024\n",
            "Epoch [6/50], Step [1000/2000], Loss: 2.3378\n",
            "Epoch [6/50], Step [1500/2000], Loss: 1.7485\n",
            "Epoch [6/50], Step [2000/2000], Loss: 2.7296\n",
            "Epoch [7/50], Step [500/2000], Loss: 1.5610\n",
            "Epoch [7/50], Step [1000/2000], Loss: 1.8396\n",
            "Epoch [7/50], Step [1500/2000], Loss: 1.7665\n",
            "Epoch [7/50], Step [2000/2000], Loss: 1.4731\n",
            "Epoch [8/50], Step [500/2000], Loss: 1.6753\n",
            "Epoch [8/50], Step [1000/2000], Loss: 2.1926\n",
            "Epoch [8/50], Step [1500/2000], Loss: 1.4999\n",
            "Epoch [8/50], Step [2000/2000], Loss: 1.6636\n",
            "Epoch [9/50], Step [500/2000], Loss: 1.5868\n",
            "Epoch [9/50], Step [1000/2000], Loss: 1.8330\n",
            "Epoch [9/50], Step [1500/2000], Loss: 2.0136\n",
            "Epoch [9/50], Step [2000/2000], Loss: 1.5945\n",
            "Epoch [10/50], Step [500/2000], Loss: 1.5262\n",
            "Epoch [10/50], Step [1000/2000], Loss: 2.1726\n",
            "Epoch [10/50], Step [1500/2000], Loss: 2.1600\n",
            "Epoch [10/50], Step [2000/2000], Loss: 1.5570\n",
            "Epoch [11/50], Step [500/2000], Loss: 1.7520\n",
            "Epoch [11/50], Step [1000/2000], Loss: 1.6824\n",
            "Epoch [11/50], Step [1500/2000], Loss: 1.6178\n",
            "Epoch [11/50], Step [2000/2000], Loss: 1.7073\n",
            "Epoch [12/50], Step [500/2000], Loss: 1.5444\n",
            "Epoch [12/50], Step [1000/2000], Loss: 1.6228\n",
            "Epoch [12/50], Step [1500/2000], Loss: 1.8118\n",
            "Epoch [12/50], Step [2000/2000], Loss: 1.6563\n",
            "Epoch [13/50], Step [500/2000], Loss: 1.9428\n",
            "Epoch [13/50], Step [1000/2000], Loss: 1.7821\n",
            "Epoch [13/50], Step [1500/2000], Loss: 1.6111\n",
            "Epoch [13/50], Step [2000/2000], Loss: 1.4866\n",
            "Epoch [14/50], Step [500/2000], Loss: 1.6082\n",
            "Epoch [14/50], Step [1000/2000], Loss: 1.5650\n",
            "Epoch [14/50], Step [1500/2000], Loss: 1.6467\n",
            "Epoch [14/50], Step [2000/2000], Loss: 1.8868\n",
            "Epoch [15/50], Step [500/2000], Loss: 1.7340\n",
            "Epoch [15/50], Step [1000/2000], Loss: 1.5312\n",
            "Epoch [15/50], Step [1500/2000], Loss: 1.8085\n",
            "Epoch [15/50], Step [2000/2000], Loss: 1.5211\n",
            "Epoch [16/50], Step [500/2000], Loss: 1.8636\n",
            "Epoch [16/50], Step [1000/2000], Loss: 1.5631\n",
            "Epoch [16/50], Step [1500/2000], Loss: 1.5825\n",
            "Epoch [16/50], Step [2000/2000], Loss: 1.6171\n",
            "Epoch [17/50], Step [500/2000], Loss: 1.5230\n",
            "Epoch [17/50], Step [1000/2000], Loss: 1.4774\n",
            "Epoch [17/50], Step [1500/2000], Loss: 1.7159\n",
            "Epoch [17/50], Step [2000/2000], Loss: 1.5556\n",
            "Epoch [18/50], Step [500/2000], Loss: 1.7938\n",
            "Epoch [18/50], Step [1000/2000], Loss: 1.6345\n",
            "Epoch [18/50], Step [1500/2000], Loss: 1.5950\n",
            "Epoch [18/50], Step [2000/2000], Loss: 1.5711\n",
            "Epoch [19/50], Step [500/2000], Loss: 1.6077\n",
            "Epoch [19/50], Step [1000/2000], Loss: 1.8697\n",
            "Epoch [19/50], Step [1500/2000], Loss: 1.7913\n",
            "Epoch [19/50], Step [2000/2000], Loss: 1.5881\n",
            "Epoch [20/50], Step [500/2000], Loss: 1.6219\n",
            "Epoch [20/50], Step [1000/2000], Loss: 1.9760\n",
            "Epoch [20/50], Step [1500/2000], Loss: 1.7551\n",
            "Epoch [20/50], Step [2000/2000], Loss: 2.1993\n",
            "Epoch [21/50], Step [500/2000], Loss: 1.4653\n",
            "Epoch [21/50], Step [1000/2000], Loss: 1.6706\n",
            "Epoch [21/50], Step [1500/2000], Loss: 1.8040\n",
            "Epoch [21/50], Step [2000/2000], Loss: 1.4954\n",
            "Epoch [22/50], Step [500/2000], Loss: 1.7052\n",
            "Epoch [22/50], Step [1000/2000], Loss: 1.6644\n",
            "Epoch [22/50], Step [1500/2000], Loss: 1.4962\n",
            "Epoch [22/50], Step [2000/2000], Loss: 1.5883\n",
            "Epoch [23/50], Step [500/2000], Loss: 1.6563\n",
            "Epoch [23/50], Step [1000/2000], Loss: 1.8311\n",
            "Epoch [23/50], Step [1500/2000], Loss: 1.7186\n",
            "Epoch [23/50], Step [2000/2000], Loss: 1.4785\n",
            "Epoch [24/50], Step [500/2000], Loss: 1.6946\n",
            "Epoch [24/50], Step [1000/2000], Loss: 1.5655\n",
            "Epoch [24/50], Step [1500/2000], Loss: 1.8338\n",
            "Epoch [24/50], Step [2000/2000], Loss: 1.6268\n",
            "Epoch [25/50], Step [500/2000], Loss: 1.4833\n",
            "Epoch [25/50], Step [1000/2000], Loss: 1.5406\n",
            "Epoch [25/50], Step [1500/2000], Loss: 1.5682\n",
            "Epoch [25/50], Step [2000/2000], Loss: 1.7970\n",
            "Epoch [26/50], Step [500/2000], Loss: 1.4566\n",
            "Epoch [26/50], Step [1000/2000], Loss: 2.1041\n",
            "Epoch [26/50], Step [1500/2000], Loss: 2.0257\n",
            "Epoch [26/50], Step [2000/2000], Loss: 1.5078\n",
            "Epoch [27/50], Step [500/2000], Loss: 1.5198\n",
            "Epoch [27/50], Step [1000/2000], Loss: 1.5302\n",
            "Epoch [27/50], Step [1500/2000], Loss: 2.2712\n",
            "Epoch [27/50], Step [2000/2000], Loss: 1.7741\n",
            "Epoch [28/50], Step [500/2000], Loss: 1.4840\n",
            "Epoch [28/50], Step [1000/2000], Loss: 1.5212\n",
            "Epoch [28/50], Step [1500/2000], Loss: 1.4737\n",
            "Epoch [28/50], Step [2000/2000], Loss: 1.5465\n",
            "Epoch [29/50], Step [500/2000], Loss: 1.4938\n",
            "Epoch [29/50], Step [1000/2000], Loss: 1.5891\n",
            "Epoch [29/50], Step [1500/2000], Loss: 1.6578\n",
            "Epoch [29/50], Step [2000/2000], Loss: 1.4845\n",
            "Epoch [30/50], Step [500/2000], Loss: 1.5896\n",
            "Epoch [30/50], Step [1000/2000], Loss: 1.8003\n",
            "Epoch [30/50], Step [1500/2000], Loss: 1.4437\n",
            "Epoch [30/50], Step [2000/2000], Loss: 2.1350\n",
            "Epoch [31/50], Step [500/2000], Loss: 1.5809\n",
            "Epoch [31/50], Step [1000/2000], Loss: 1.6342\n",
            "Epoch [31/50], Step [1500/2000], Loss: 1.4440\n",
            "Epoch [31/50], Step [2000/2000], Loss: 1.4395\n",
            "Epoch [32/50], Step [500/2000], Loss: 1.5938\n",
            "Epoch [32/50], Step [1000/2000], Loss: 2.9795\n",
            "Epoch [32/50], Step [1500/2000], Loss: 1.8984\n",
            "Epoch [32/50], Step [2000/2000], Loss: 1.9696\n",
            "Epoch [33/50], Step [500/2000], Loss: 1.5261\n",
            "Epoch [33/50], Step [1000/2000], Loss: 1.4893\n",
            "Epoch [33/50], Step [1500/2000], Loss: 1.4903\n",
            "Epoch [33/50], Step [2000/2000], Loss: 1.6438\n",
            "Epoch [34/50], Step [500/2000], Loss: 1.5602\n",
            "Epoch [34/50], Step [1000/2000], Loss: 1.9762\n",
            "Epoch [34/50], Step [1500/2000], Loss: 1.4818\n",
            "Epoch [34/50], Step [2000/2000], Loss: 1.6734\n",
            "Epoch [35/50], Step [500/2000], Loss: 1.7745\n",
            "Epoch [35/50], Step [1000/2000], Loss: 1.4971\n",
            "Epoch [35/50], Step [1500/2000], Loss: 1.9538\n",
            "Epoch [35/50], Step [2000/2000], Loss: 1.4630\n",
            "Epoch [36/50], Step [500/2000], Loss: 1.7724\n",
            "Epoch [36/50], Step [1500/2000], Loss: 1.8053\n",
            "Epoch [36/50], Step [2000/2000], Loss: 1.6183\n",
            "Epoch [37/50], Step [500/2000], Loss: 1.7652\n",
            "Epoch [37/50], Step [1000/2000], Loss: 1.4994\n",
            "Epoch [37/50], Step [1500/2000], Loss: 1.7347\n",
            "Epoch [37/50], Step [2000/2000], Loss: 1.7177\n",
            "Epoch [38/50], Step [500/2000], Loss: 1.8370\n",
            "Epoch [38/50], Step [1000/2000], Loss: 2.1095\n",
            "Epoch [38/50], Step [1500/2000], Loss: 1.7437\n",
            "Epoch [38/50], Step [2000/2000], Loss: 1.7453\n",
            "Epoch [39/50], Step [500/2000], Loss: 1.6185\n",
            "Epoch [39/50], Step [1000/2000], Loss: 1.6436\n",
            "Epoch [39/50], Step [1500/2000], Loss: 1.4530\n",
            "Epoch [39/50], Step [2000/2000], Loss: 1.5355\n",
            "Epoch [40/50], Step [500/2000], Loss: 1.5137\n",
            "Epoch [40/50], Step [1000/2000], Loss: 1.5385\n",
            "Epoch [40/50], Step [1500/2000], Loss: 1.5717\n",
            "Epoch [40/50], Step [2000/2000], Loss: 1.6863\n",
            "Epoch [41/50], Step [500/2000], Loss: 1.5880\n",
            "Epoch [41/50], Step [1000/2000], Loss: 1.4434\n",
            "Epoch [41/50], Step [1500/2000], Loss: 1.7664\n",
            "Epoch [41/50], Step [2000/2000], Loss: 1.8133\n",
            "Epoch [42/50], Step [500/2000], Loss: 1.4455\n",
            "Epoch [42/50], Step [1000/2000], Loss: 1.8192\n",
            "Epoch [42/50], Step [1500/2000], Loss: 1.5259\n",
            "Epoch [42/50], Step [2000/2000], Loss: 1.8435\n",
            "Epoch [43/50], Step [500/2000], Loss: 1.5229\n",
            "Epoch [43/50], Step [1000/2000], Loss: 1.6594\n",
            "Epoch [43/50], Step [1500/2000], Loss: 1.7982\n",
            "Epoch [43/50], Step [2000/2000], Loss: 1.7077\n",
            "Epoch [44/50], Step [500/2000], Loss: 1.4780\n",
            "Epoch [44/50], Step [1000/2000], Loss: 1.4610\n",
            "Epoch [44/50], Step [1500/2000], Loss: 1.5577\n",
            "Epoch [44/50], Step [2000/2000], Loss: 1.4555\n",
            "Epoch [45/50], Step [500/2000], Loss: 1.7401\n",
            "Epoch [45/50], Step [1000/2000], Loss: 2.1719\n",
            "Epoch [45/50], Step [1500/2000], Loss: 1.4389\n",
            "Epoch [45/50], Step [2000/2000], Loss: 1.4535\n",
            "Epoch [46/50], Step [500/2000], Loss: 1.4972\n",
            "Epoch [46/50], Step [1000/2000], Loss: 1.8414\n",
            "Epoch [46/50], Step [1500/2000], Loss: 1.7869\n",
            "Epoch [46/50], Step [2000/2000], Loss: 1.5306\n",
            "Epoch [47/50], Step [500/2000], Loss: 1.7544\n",
            "Epoch [47/50], Step [1000/2000], Loss: 1.5348\n",
            "Epoch [47/50], Step [1500/2000], Loss: 1.8057\n",
            "Epoch [47/50], Step [2000/2000], Loss: 1.6976\n",
            "Epoch [48/50], Step [500/2000], Loss: 1.4707\n",
            "Epoch [48/50], Step [1000/2000], Loss: 1.5268\n",
            "Epoch [48/50], Step [1500/2000], Loss: 1.8305\n",
            "Epoch [48/50], Step [2000/2000], Loss: 1.5836\n",
            "Epoch [49/50], Step [500/2000], Loss: 1.4409\n",
            "Epoch [49/50], Step [1000/2000], Loss: 1.6645\n",
            "Epoch [49/50], Step [1500/2000], Loss: 1.6279\n",
            "Epoch [49/50], Step [2000/2000], Loss: 1.5348\n",
            "Epoch [50/50], Step [500/2000], Loss: 1.9599\n",
            "Epoch [50/50], Step [1000/2000], Loss: 1.7243\n",
            "Epoch [50/50], Step [1500/2000], Loss: 1.5645\n",
            "Epoch [50/50], Step [2000/2000], Loss: 1.4728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daUGjb7EgCH8"
      },
      "source": [
        "## **Model Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe7XVtEJgLVz"
      },
      "source": [
        "def validate(data_loader):\n",
        "    correct = 0\n",
        "    correctNums = 0\n",
        "    correctLetters = 0\n",
        "    total = 0\n",
        "\n",
        "  # do not need to keep track of gradients for validation \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            images = images.reshape(batch_size, 1, 56, 56)\n",
        "          # calculate outputs by running images through the network\n",
        "            outputs = model(images.float())\n",
        "          # the class with the highest energy is what we choose as prediction\n",
        "          #_, predicted = torch.max(outputs.data, 1)\n",
        "            numbers = outputs.data[:, :10]\n",
        "            _, predictedNums = torch.max(numbers, 1)\n",
        "            _, actualNums = torch.max(labels[:, :10], 1)\n",
        "\n",
        "            letters = outputs.data[:, 10:]\n",
        "            \n",
        "            _, predictedLetters = torch.max(letters, 1)\n",
        "            _, actualLetters = torch.max(labels[:, 10:], 1)\n",
        "\n",
        "            correctNums += (predictedNums == actualNums).sum().item()\n",
        "\n",
        "            correctLetters += (predictedLetters == actualLetters).sum().item()\n",
        "\n",
        "            total += predictedNums.shape[0]\n",
        "            correct += ((predictedNums == actualNums) & (predictedLetters == actualLetters)).sum().item()\n",
        "\n",
        "        ovr_acc = correct / total\n",
        "        num_acc = correctNums / total\n",
        "        let_acc = correctLetters / total\n",
        "\n",
        "        print(f'=====> Total Accuracy: {ovr_acc:.4f}\\t'\n",
        "              f'Number Accuracy: {num_acc:.4f}\\t'\n",
        "              f'Letter Accuracy: {let_acc:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "758tqkERgm1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0ff87e-43e8-4c8a-8157-5252633800a3"
      },
      "source": [
        "print(\"Training Scores:\")\n",
        "validate(train_loader)\n",
        "print(\"Validation Scores:\")\n",
        "validate(val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Scores:\n",
            "=====> Total Accuracy: 0.9241\tNumber Accuracy: 0.9739\tLetter Accuracy: 0.94\n",
            "Validation Scores:\n",
            "=====> Total Accuracy: 0.8420\tNumber Accuracy: 0.9323\tLetter Accuracy: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow1uL0S1jMOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10588634-2981-44fb-ec6c-0a7262baebb8"
      },
      "source": [
        "print(\"Training Scores:\")\n",
        "validate(train_loader)\n",
        "print(\"Validation Scores:\")\n",
        "validate(val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Scores:\n",
            "=====> Total Accuracy: 0.9137\tNumber Accuracy: 0.9667\tLetter Accuracy: 0.93\n",
            "Validation Scores:\n",
            "=====> Total Accuracy: 0.8993\tNumber Accuracy: 0.9598\tLetter Accuracy: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58A7gQN0gt28"
      },
      "source": [
        "## **Call model on Test data for submission**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhexqrXFg08y"
      },
      "source": [
        "def test(data_loader):\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "  # do not need to keep track of gradients for validation \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "\n",
        "            images = data\n",
        "           \n",
        "            images = images.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            numbers = outputs.data[:, :10]\n",
        "            _, predictedNums = torch.max(numbers, 1)\n",
        "\n",
        "            letters = outputs.data[:, 10:]\n",
        "            _, predictedLetters = torch.max(letters, 1)\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(len(outputs)): #len(outputs)\n",
        "\n",
        "              answer = np.zeros(36,dtype=np.int8)\n",
        "\n",
        "              answer[predictedNums[i]] = 1\n",
        "              answer[predictedLetters[i]+10] = 1 \n",
        "\n",
        "              predictions.append(answer)\n",
        "              \n",
        "  \n",
        "    # convert predictions into a strings of 1,0s for submission\n",
        "    string_predictions = []\n",
        "    for current_array in predictions:\n",
        "      current_array = ''.join(map(str,current_array))\n",
        "      string_predictions.append(current_array)\n",
        "\n",
        "    return string_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNkKEb1hLZy"
      },
      "source": [
        "# load data\n",
        "test_path = \"/content/drive/My Drive/Colab Notebooks/images_test.pkl\"\n",
        "with open(test_path, 'rb') as f: \n",
        "    test_data = pickle.load(f)\n",
        "# reshape data to tensors and correct channels\n",
        "test_data = test_data.reshape(15000,1,56,56)\n",
        "# create dataset and dataloaders\n",
        "test_data = torch.from_numpy(test_data).float().to(device)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "test_output = test(testloader)\n",
        "# create dataframe to output as csv\n",
        "test_df = pd.DataFrame(test_output,columns=['Category'])\n",
        "test_df.index.name = '# Id'\n",
        "test_df.to_csv('pls_work.csv',index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKWCuFbbcWZC"
      },
      "source": [
        "## **Helper Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDXElTdKcaYq"
      },
      "source": [
        "### Visualize data points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62HEZDzoccES"
      },
      "source": [
        "def peek(tensor, nm=(2,2), labels=None):\n",
        "\n",
        "  def interpret(label):\n",
        "    idxs = np.where(label == 1)[0]\n",
        "    number = idxs[0]\n",
        "    alphabet = chr(idxs[1] - 10 + 97)\n",
        "    return (number, alphabet)\n",
        "  \n",
        "  tensor = tensor.squeeze(1).cpu()\n",
        "  verbose = True if labels is not None else False\n",
        "  if verbose:\n",
        "    labels = labels.cpu()\n",
        "  n, m = nm\n",
        "  f, ax = plt.subplots(n, m, figsize=(16,16))\n",
        "  for i in range(0, n*m):\n",
        "    ax[i//m, i%m].imshow(tensor[i].numpy())\n",
        "    if verbose:\n",
        "      ax[i//m, i%m].set_title(f'{interpret(labels[i])}')\n",
        "  plt.show()\n",
        "\n",
        "peek(X_train, nm=(3,3), labels=y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWPsiYsghWo7"
      },
      "source": [
        "### Convert data to binary using thresholding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzEsIocshZi4"
      },
      "source": [
        "def image_to_binary(input_images):\n",
        "\n",
        "  binary_images = np.zeros(np.shape(input_images))\n",
        "  for i in range(input_images.shape[0]):\n",
        "    binary_images[i] = input_images[i][:, :] > 125 \n",
        "\n",
        "  return binary_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e5HZZLv7IV_"
      },
      "source": [
        "## **Playing around with unlabelled data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkYII4807Rqz"
      },
      "source": [
        "### Get results for unlabelled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1hp4Wv7SOk"
      },
      "source": [
        "# model.eval()\n",
        "def unlabeled(data_loader):\n",
        "\n",
        "    unlabeled_images = []\n",
        "    unlabeled_labels = []\n",
        "\n",
        "  # do not need to keep track of gradients for validation \n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "\n",
        "            images = data\n",
        "           \n",
        "            images = images.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            numbers = outputs.data[:, :10]\n",
        "            _, predictedNums = torch.max(numbers, 1)\n",
        "\n",
        "            letters = outputs.data[:, 10:]\n",
        "            _, predictedLetters = torch.max(letters, 1)\n",
        "\n",
        "\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "              #print(f\"Iteration # {i}\")\n",
        "              #check if two letters or numbers are +ve, if they are we ignore current unlabelled example\n",
        "              number_of_positives_numbers = 0\n",
        "              numbers_of_positive_letters = 0\n",
        "              for number_pred in numbers[i]:\n",
        "                number_pred = number_pred.item()\n",
        "                if (number_pred>0):\n",
        "                  #print(f\"Number_pred:{number_pred}\")\n",
        "                  number_of_positives_numbers = number_of_positives_numbers + 1\n",
        "\n",
        "              for letter_pred in letters[i]:\n",
        "                letter_pred = letter_pred.item()\n",
        "                if (letter_pred>0):\n",
        "                  #print(f\"Letter_pred:{letter_pred}\")\n",
        "                  numbers_of_positive_letters = numbers_of_positive_letters + 1\n",
        "\n",
        "              \n",
        "              # print(\"Number of positives:\", number_of_positives)\n",
        "              \n",
        "\n",
        "              if (number_of_positives_numbers > 2 or numbers_of_positive_letters > 2):\n",
        "                continue\n",
        "              else:\n",
        "                # print(\"We should include this:\")\n",
        "                # print(\"predicted nums:\",predictedNums[i],numbers[i])\n",
        "                # print(\"predicted letters:\",predictedLetters[i], letters[i])\n",
        "\n",
        "                label = np.zeros(36,dtype=np.float32)  \n",
        "                label[predictedNums[i]] = 1\n",
        "                label[predictedLetters[i]+10] = 1  \n",
        "                unlabeled_labels.append(label)\n",
        "                unlabeled_images.append(images[i])\n",
        "\n",
        "\n",
        "    return unlabeled_images, unlabeled_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hV_AZhS7UYv"
      },
      "source": [
        "unlabeled_path = \"/content/drive/My Drive/Colab Notebooks/images_ul.pkl\"\n",
        "with open(unlabeled_path, 'rb') as f: \n",
        "    unlabeled_data = pickle.load(f)\n",
        "\n",
        "unlabeled_data = unlabeled_data.reshape(30000,1,56,56)\n",
        "unlabeled_data = torch.from_numpy(unlabeled_data).float().to(device)\n",
        "unlabeled_data = torch.utils.data.DataLoader(unlabeled_data, batch_size=batch_size)\n",
        "unlabeled_images, unlabeled_labels = unlabeled(unlabeled_data)\n",
        "\n",
        "# convert list of images (unlabeled data) to single tensor\n",
        "unlabeled_images_tensor = torch.Tensor(len(unlabeled_images), 56, 56).to(device)\n",
        "torch.cat(unlabeled_images, out=unlabeled_images_tensor)\n",
        "unlabeled_images = unlabeled_images_tensor.view(len(unlabeled_images),1,56,56)\n",
        "# convert new labels to tensor\n",
        "unlabeled_labels = np.stack(unlabeled_labels, axis=0)\n",
        "unlabeled_labels = torch.from_numpy(unlabeled_labels)\n",
        "print(unlabeled_images.shape, unlabeled_labels.shape)\n",
        "\n",
        "# create dataset and dataloaders for newly labeled data\n",
        "newly_labeled = torch.utils.data.TensorDataset(unlabeled_images,unlabeled_labels)\n",
        "new_label_dataloader = torch.utils.data.DataLoader(newly_labeled, batch_size=batch_size, shuffle=True) # look into numworkers\n",
        "\n",
        "# train using newly labeled dataset\n",
        "train_model(new_label_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei--22Pd8-WA"
      },
      "source": [
        "print(\"After incorporating unlabelled data:\")\n",
        "print(\"Training Scores:\")\n",
        "validate(train_loader)\n",
        "print(\"Validation Scores:\")\n",
        "validate(val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}